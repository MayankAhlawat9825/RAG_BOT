{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d11b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4973786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the api and setting it\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = genai.configure(api_key=api_key)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ad7b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# result = client.models.embed_content(\n",
    "#     model=\"gemini-embedding-001\",\n",
    "#     contents=[\"What is the meaning of life?\"],\n",
    "#     config=types.EmbedContentConfig(task_type=\"SEMANTIC_SIMILARITY\"))\n",
    "\n",
    "# for embedding in result.embeddings:\n",
    "#     print(embedding)\n",
    "# print(result.embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a89acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Cosine similarity check between two sentences(like Query and retreived Portion)\n",
    "text_1 =\"cat sits on mat\"\n",
    "text_2 = \"dog lies on rug\"\n",
    "text_3 = \"how do I bake a cake\"\n",
    "\n",
    "model = \"gemini-embedding-001\"\n",
    "\n",
    "def embed_text(text,model_name):\n",
    "    response = client.models.embed_content(\n",
    "        model=model_name,\n",
    "        contents=text,\n",
    "        config=types.EmbedContentConfig(task_type=\"SEMANTIC_SIMILARITY\"))\n",
    "    return np.array(response.embeddings[0].values)\n",
    "\n",
    "embedding_1 = embed_text(text_1,model)\n",
    "embedding_2 = embed_text(text_2,model)\n",
    "embedding_3 = embed_text(text_3,model)\n",
    "\n",
    "def cosine_similarity (embedding1,embedding2):\n",
    "    return np.dot(embedding1,embedding2) / ((np.linalg.norm(embedding1))*(np.linalg.norm(embedding2)))\n",
    "\n",
    "similarity_1 = cosine_similarity(embedding_1,embedding_2)\n",
    "similarity_2 = cosine_similarity(embedding_1,embedding_3)\n",
    "\n",
    "print(f\"the similarity of {text_1} and {text_2} is {similarity_1:.4f}\")\n",
    "print(f\"the similarity of {text_1} and {text_3} is {similarity_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb2574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the sentence Transformer for embedding the given text\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"My home is far away from here\",\n",
    "    \"i live next to this house\",\n",
    "    \"i love orange\",\n",
    "    \"i like to go on a vacation\"\n",
    "]\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "# [3, 384]\n",
    "\n",
    "query=\"How far do you stay\"\n",
    "\n",
    "query_embeddings = model.encode(query)\n",
    "print(query_embeddings.shape)\n",
    "\n",
    "# 3. Calculate the embedding similarities\n",
    "similarities = model.similarity(embeddings, query_embeddings)\n",
    "print(similarities)\n",
    "# tensor([[1.0000, 0.6660, 0.1046],\n",
    "#         [0.6660, 1.0000, 0.1411],\n",
    "#         [0.1046, 0.1411, 1.0000]])\n",
    "\n",
    "similar = model.similarity(model.encode(\"cat\") , model.encode(\"kitty\"))\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cea8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the sentence Transformer embedding the given text and storing it in the ChromaDB\n",
    "# and query it and retrieve the relevant info\n",
    "\n",
    "chroma_Client = chromadb.Client()\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "collection = chroma_Client.get_or_create_collection(name=\"my_collection\")\n",
    "\n",
    "documents=[\n",
    "    \"Artificial intelligence is transforming healthcare.\",\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"Machine learning models can be used for predictions.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Eiffel Tower is one of the most famous landmarks in Paris.\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(documents)\n",
    "\n",
    "collection.upsert(\n",
    "    documents=documents,\n",
    "    embeddings=embeddings,\n",
    "    ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
    ")\n",
    "query = \"Tell me about Paris.\"\n",
    "\n",
    "query_embedding=model.encode(query)\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding,\n",
    "    # n_results=3\n",
    ")\n",
    "\n",
    "print(\"Query: \",query)\n",
    "print(\"Top Matches are\")\n",
    "for doc , scores in zip(results[\"documents\"][0],results[\"distances\"][0]):\n",
    "    print(f\"- {doc} (distance:{scores:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "057ca1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document based basic embedding and query\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ae7b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_doc(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text=\"\"\n",
    "\n",
    "    for page in reader.pages:\n",
    "        text+= page.extract_text() + \"/n\"\n",
    "    return text\n",
    "document_text = extract_text_from_doc(\"./Data/Ai.pdf\")\n",
    "# print(document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17798380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text,chunk_size=100 , overlap=50):\n",
    "    words = text.split()\n",
    "    chunks=[]\n",
    "    start=0\n",
    "\n",
    "    while start <= len(words):\n",
    "        end = min(start+chunk_size , len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start +=chunk_size-overlap\n",
    "\n",
    "    return chunks\n",
    "chunks = chunk_text(document_text)\n",
    "# print(len(chunks))\n",
    "# print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea48f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.encode(chunks).tolist()\n",
    "\n",
    "collection.upsert(\n",
    "    documents=chunks,\n",
    "    embeddings=embedding,\n",
    "    ids= [f\"doc_{i}\" for i in range(len(chunks))]\n",
    ")\n",
    "\n",
    "# for doc , score in zip(result_doc[\"documents\"][0] , result_doc[\"distances\"][0]):\n",
    "#     print(f\"- {doc} (distance: {score:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7744d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot: There is no single, universally accepted definition for Artificial Intelligence. However, the Oxford English Dictionary defines AI as “the capacity of computers, or other machines, to exhibit intelligent behaviour”. This means AI systems appear to think, learn and act like humans, and in some cases can exceed human capabilities. AI systems can analyse vast amounts of data, solve complex problems, make decisions, and perform creative tasks.\n",
      "ChatBot: This report aims to provide a general introduction to Artificial Intelligence (AI) technology, its business applications, and its relevance to Travel & Tourism. While AI has gained significant attention in recent years, especially in 2023, it is not new. Its history can be traced back to the development of computers after the Second World War, with the Dartmouth Conference in 1956 bringing together researchers to explore \"thinking machines.\" Today, it is almost impossible to browse news or social media without seeing mention of AI, with major magazines dedicating cover stories to it in 2023.\n",
      "ChatBot: Based on the provided context:\n",
      "\n",
      "The context does not state who invented AI. However, it mentions that Alan Turing published a groundbreaking paper in 1950 which introduced the ‘Turing Test’ for artificial intelligence. It also states that the term “Artificial Intelligence” was used for the first time by the visionaries at the Dartmouth Conference in 1956, which is widely considered the start of AI as a distinct field of study.\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# integrate the LLM into the document chunk retrieval based on query process\n",
    "\n",
    "import os\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "from google.genai import types\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "# genration_config = {\n",
    "#     \"temprature\":0.7,\n",
    "#     \"max_output_token\":100\n",
    "# }\n",
    "\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "\n",
    "\n",
    "    if query.lower() in {\"quit\" , \"exit\" , \"break\"}:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    query_embedded = model.encode(query)\n",
    "\n",
    "    result_doc = collection.query(\n",
    "        query_embeddings=query_embedded,\n",
    "        n_results=3\n",
    "    )\n",
    "\n",
    "    retrieved_chunks=result_doc[\"documents\"][0] \n",
    "    context= \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "\n",
    "    prompt=f\"\"\"\n",
    "    You are a helpful Assistant.\n",
    "    Answer the following query based only on the provided context.\n",
    "    \n",
    "    query={query}\n",
    "\n",
    "    Context:{context}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.7,\n",
    "            # max_output_tokens=5\n",
    "        ),\n",
    "        contents=prompt\n",
    "    )\n",
    "\n",
    "    print(f\"ChatBot: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6071489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
